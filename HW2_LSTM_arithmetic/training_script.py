# %% [markdown]
# # LSTM-arithmetic
# # LSTM-arithmetic (Corrected)
# 
# ## Dataset
# - [Arithmetic dataset](https://drive.google.com/file/d/1cMuL3hF9jefka9RyF4gEBIGGeFGZYHE-/view?usp=sharing)

# %%
## AI disclaimer: The whole script has been generated by AI
import numpy as np
import pandas as pd
import torch
import torch.nn
import torch.nn.utils.rnn
import torch.utils.data
import os
import argparse
import sys
from pathlib import Path

# Ensure data directory exists
data_path = './data'
os.makedirs(data_path, exist_ok=True)

# --- Argument Parsing for Checkpoint Directory ---
parser = argparse.ArgumentParser(description='Train an LSTM for arithmetic tasks.')
parser.add_argument('--log_file', type=str, default='checkpoints', 
                    help='Name of the log file, used to create the checkpoint directory.')

# In a notebook environment, args might not be parsed from command line.
args, _ = parser.parse_known_args()

log_dir_name = args.log_file.replace('.log', '') if args.log_file.endswith('.log') else args.log_file
CHECKPOINT_DIR = Path("checkpoints") / log_dir_name

# %%
# Load data
## AI disclaimer: The whole script has been generated by AI
df_train = pd.read_csv(os.path.join(data_path, 'arithmetic_train.csv'))
df_eval = pd.read_csv(os.path.join(data_path, 'arithmetic_eval.csv'))
df_train.head()

# %% [markdown]
# ### Data Preparation
# This is the most critical step. We add `<eos>` to the end of the full sequence *before* creating the shifted input/target. This ensures the model learns the `stop` signal correctly.

# %%
# --- FINAL CORRECTED LOGIC ---
df_train['tgt'] = df_train['tgt'].apply(str)
# We will handle tokenization and <eos> in the preprocessing step
df_train['full_sequence_text'] = df_train['src'] + df_train['tgt']

# Apply the same logic to the evaluation set for a fair comparison
df_eval['tgt'] = df_eval['tgt'].apply(str)
df_eval['full_answer'] = df_eval['src'] + df_eval['tgt'] + '<eos>'
df_eval['prompt'] = df_eval['src']

df_train[['full_sequence_text']].head()


# %% [markdown]
# ### Build Dictionary
# The dictionary is now built from the raw `src` and `tgt` strings, and special tokens are added manually. This prevents `<eos>` from being split.

# %%
## AI disclaimer: The whole script has been generated by AI
# --- BUG FIX: Build dictionary from raw characters only ---
char_to_id = {'<pad>': 0, '<eos>': 1} # Manually define all special tokens
char_id_counter = 2

# Build dictionary from the raw arithmetic parts to avoid splitting '<eos>'
for seq in (df_train['src'] + df_train['tgt']):
    for char in seq:
        if char not in char_to_id:
            char_to_id[char] = char_id_counter
            char_id_counter += 1
            
id_to_char = {v: k for k, v in char_to_id.items()}
vocab_size = len(char_to_id)

# Calculate max length for padding (based on token count, not string length)
max_len_for_padding = (df_train['src'] + df_train['tgt']).str.len().max() + 1 # +1 for <eos>

print(f"Max token sequence length: {max_len_for_padding}")
print(f"Vocab size: {vocab_size}")
print("Character to ID Mapping:", char_to_id)

# %% [markdown]
# ### Data Preprocessing
# This has been completely rewritten to be token-based and logically sound.

# %%
# --- FINAL CORRECTED PREPROCESSING ---
## AI disclaimer: The whole script has been generated by AI
def create_and_pad_sequences(text, max_len, mapping):
    """
    Creates shifted input/target token lists, pads them, and converts to IDs.
    This is the core of the corrected logic.
    """
    # 1. Create the full token list
    tokens = list(text) + ['<eos>']
    
    # 2. Create shifted sequences from the token list
    input_tokens = tokens[:-1]
    target_tokens = tokens[1:]
    
    # 3. Pad both sequences to the same max length
    input_padding = ['<pad>'] * (max_len - len(input_tokens))
    target_padding = ['<pad>'] * (max_len - len(target_tokens))
    
    padded_input = input_tokens + input_padding
    padded_target = target_tokens + target_padding
    
    # 4. Convert to IDs
    input_ids = [mapping[token] for token in padded_input]
    target_ids = [mapping[token] for token in padded_target]
    
    return input_ids, target_ids

# Apply preprocessing to training data
processed_sequences = df_train['full_sequence_text'].apply(
    lambda s: create_and_pad_sequences(s, max_len_for_padding, char_to_id)
)

# Unpack the tuple of (input_ids, target_ids) into two new columns
df_train['src_list_id'] = [item[0] for item in processed_sequences]
df_train['tgt_list_id'] = [item[1] for item in processed_sequences]

df_train[['src_list_id', 'tgt_list_id']].head()

# %% [markdown]
# ### Hyper Parameters
# Increased `EMBED_DIM` for better model capacity.

BATCH_SIZE = 1024
EPOCHS = 30
LR = 0.001
GRAD_CLIP = 1.0

# nohup python -u training_script.py --log_file train_GRU.log > train_GRU.log &


# %% [markdown]
# Import Model and Hyperparameters
## AI disclaimer: The whole script has been generated by AI
# from model import CharRNN, EMBED_DIM, HIDDEN_DIM
EMBED_DIM = 128
HIDDEN_DIM = 256

# --- Model Definition ---
class CharRNN(torch.nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, padding_idx):
        super(CharRNN, self).__init__()
        self.embedding = torch.nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)
        self.rnn_layer1 = torch.nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.rnn_layer2 = torch.nn.LSTM(hidden_dim, hidden_dim, batch_first=True)
        self.linear = torch.nn.Sequential(
            torch.nn.Linear(hidden_dim, hidden_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, vocab_size)
        )

    def forward(self, batch_x, batch_x_lens):
        embedded = self.embedding(batch_x)
        # Clamp lengths to be at least 1, as pack_padded_sequence requires non-zero lengths
        clamped_lens = torch.clamp(batch_x_lens, min=1)
        packed = torch.nn.utils.rnn.pack_padded_sequence(
            embedded, clamped_lens.cpu(), batch_first=True, enforce_sorted=False
        )
        packed_out, _ = self.rnn_layer1(packed)
        packed_out, _ = self.rnn_layer2(packed_out)
        unpacked_out, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)
        logits = self.linear(unpacked_out)
        return logits
# %% [markdown]
# ### Data Batching
# Create PyTorch Dataset and DataLoader.

# %%
## AI disclaimer: The whole script has been generated by AI
class ArithmeticDataset(torch.utils.data.Dataset):
    def __init__(self, dataframe):
        self.dataframe = dataframe
    
    def __len__(self):
        return len(self.dataframe)
    
    def __getitem__(self, index):
        x = self.dataframe['src_list_id'].iloc[index]
        y = self.dataframe['tgt_list_id'].iloc[index]
        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)

def collate_fn(batch):
    batch_x, batch_y = zip(*batch)
    
    # Correctly calculate true lengths by counting non-pad tokens
    batch_x_lens = torch.tensor([torch.sum(x != char_to_id['<pad>']) for x in batch_x], dtype=torch.long)
    
    # Since data is already padded, we can just stack it
    stacked_x = torch.stack(batch_x)
    stacked_y = torch.stack(batch_y)
    
    return stacked_x, stacked_y, batch_x_lens

# %%

ds_train = ArithmeticDataset(df_train)
dl_train = torch.utils.data.DataLoader(ds_train, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)

# %% [markdown]
# ### Model Design
# Standard CharRNN model.
## AI disclaimer: The whole script has been generated by AI
def generator(model, start_char, max_len, char_to_id, id_to_char):
    """
    Generates a sequence from a model given a starting prompt.
    """
    generated_string = start_char
    model.eval()
    with torch.no_grad():
        for _ in range(max_len):
            char_ids = [char_to_id[c] for c in generated_string]
            input_tensor = torch.tensor(char_ids, dtype=torch.long).unsqueeze(0)
            input_tensor = input_tensor.to(next(model.parameters()).device)

            embedded = model.embedding(input_tensor)
            rnn_out, _ = model.rnn_layer1(embedded)
            rnn_out, _ = model.rnn_layer2(rnn_out)
            last_token_logits = model.linear(rnn_out[:, -1, :])

            next_char_id = torch.argmax(last_token_logits, dim=1).item()
            next_char = id_to_char[next_char_id]

            generated_string += next_char

            if next_char == '<eos>':
                break
    return generated_string

# %%


# %%
padding_idx = char_to_id['<pad>']
# Setup for Training
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = CharRNN(vocab_size, EMBED_DIM, HIDDEN_DIM, padding_idx).to(device)
criterion = torch.nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'])
optimizer = torch.optim.AdamW(model.parameters(), lr=LR)
print(f"Using device: {device}")

# %% [markdown]
# # Training
# 1. The outer `for` loop controls the `epoch`
#     1. The inner `for` loop uses `data_loader` to retrieve batches.
#         1. Pass the batch to the `model` for training.
#         2. Compare the predicted results `batch_pred_y` with the true labels `batch_y` using Cross Entropy to calculate the loss `loss`
#         3. Use `loss.backward` to automatically compute the gradients.
#         4. Use `torch.nn.utils.clip_grad_value_` to limit the gradient values between `-grad_clip` &lt; and &lt; `grad_clip`.
#         5. Use `optimizer.step()` to update the model (backpropagation).
# 2.  After every `1000` batches, output the current loss to monitor whether it is converging.
# ### Training & Evaluation Loop
# The training loop with a corrected evaluation function.

# %%
from tqdm import tqdm
import os
## AI disclaimer: The whole script has been generated by AI
# Create checkpoints directory from parsed argument
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

for epoch in range(1, EPOCHS + 1):
    model.train()
    total_loss = 0
    
    bar = tqdm(dl_train, desc=f"Train Epoch {epoch}", disable=not sys.stdout.isatty())
    for batch_x, batch_y, batch_x_lens in bar:
        optimizer.zero_grad()
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        predictions = model(batch_x, batch_x_lens)
        
        # Reshape for loss calculation
        # Slice target to match the dynamic length of the unpacked prediction
        target_for_loss = batch_y[:, :predictions.shape[1]]
        loss = criterion(predictions.reshape(-1, vocab_size), target_for_loss.reshape(-1))
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP) # Use norm clipping
        optimizer.step()
        
        total_loss += loss.item()
        bar.set_postfix(loss=f"{loss.item():.4f}")

    print(f"Epoch {epoch} Average Training Loss: {total_loss / len(dl_train):.4f}")

    # Generated by GEMINI: (AI usage disclosure)
    # --- CORRECTED EVALUATION ---
    model.eval()
    matched = 0
    incorrect_predictions = []
    # Evaluate on a sample to speed things up
    eval_sample = df_eval.sample(n=1000, random_state=epoch) 
    
    bar_eval = tqdm(eval_sample.iterrows(), total=len(eval_sample), desc=f"Validation Epoch {epoch}", disable=not sys.stdout.isatty())
    with torch.no_grad():
        for i, (_, row) in enumerate(bar_eval):
            prompt = row['prompt']
            ground_truth = row['full_answer']
            
            # Generate the full sequence
            prediction = generator(model, prompt, 50, char_to_id, id_to_char)
            
            # Check if the generated sequence is an exact match
            if prediction == ground_truth:
                matched += 1
            elif len(incorrect_predictions) < 100: # Log some incorrect examples
                incorrect_predictions.append(f"Input: {prompt}\nGround Truth: {ground_truth}\nPrediction: {prediction}\n")

            bar_eval.set_postfix(accuracy=f"{(matched / (bar_eval.n + 1)):.4f}")
    
    em_accuracy = matched / len(eval_sample)
    print(f"Validation Epoch {epoch} | Exact Match Accuracy: {em_accuracy:.4f}")

    # Save checkpoint
    checkpoint_path = os.path.join(CHECKPOINT_DIR, f"model_epoch_{epoch}.pth")
    torch.save(model.state_dict(), checkpoint_path)
    print(f"Checkpoint for epoch {epoch} saved to {checkpoint_path}")

    # Save some incorrect predictions for analysis
    if incorrect_predictions:
        with open(os.path.join(CHECKPOINT_DIR, f"incorrect_preds_epoch_{epoch}.log"), "w") as f:
            f.writelines(incorrect_predictions)

# %%

# %%


# %%
